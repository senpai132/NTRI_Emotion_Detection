{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "from emot.emo_unicode import UNICODE_EMOJI, UNICODE_EMOJI_ALIAS, EMOTICONS_EMO\n",
    "from flashtext import KeywordProcessor\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import train_test_split\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(record):\n",
    "    return word_tokenize(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemovePuncuation(record):\n",
    "   return [word for word in record if word.isalpha()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(record):\n",
    "    return [word.lower() for word in record if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUrls(record, httpUrls, noHttpUrls):\n",
    "    return [word for word in record if re.search(httpUrls, word) is None and re.search(noHttpUrls, word) is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatize(record, lemmatizer):\n",
    "    return map(lemmatizer.lemmatize, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveContractions(record):\n",
    "    temp = [] \n",
    "    for word in record.split():\n",
    "        temp.append(contractions.fix(word))\n",
    "    return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnwrapSlangExpressions(record, chat_expressions_dict):\n",
    "    return re.sub(r'\\S+', lambda m: chat_expressions_dict.get(m.group().upper(), m.group()) , record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertEmojisAndEmoticonsToText(record, all_emoji_and_emoticons):\n",
    "    return all_emoji_and_emoticons.replace_keywords(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRareAndFrequentWords(dataFrame, minThreshhold = 0, maxThreshhold = 1000000):\n",
    "    counts = Counter()\n",
    "    dataFrame['Text'].str.lower().str.split().apply(counts.update)\n",
    "\n",
    "    frequentWords = [word for word in counts if counts[word] >= maxThreshhold]\n",
    "    rareWords = [word for word in counts if counts[word] <= minThreshhold]\n",
    "    return rareWords, frequentWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveRareAndFrequentWords(record, rareWords, frequentWords):\n",
    "    return [word for word in record if word not in rareWords and word not in frequentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNonEnglishRecord(record):\n",
    "    if(record):\n",
    "        return record if detect(record) == 'en' else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "httpUrls = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "noHttpUrls = \"^[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "chat_expressions_url = \"https://raw.githubusercontent.com/MFuchs1989/Datasets-and-Miscellaneous/main/datasets/NLP/Text%20Pre-Processing%20VII%20(Special%20Cases)/chat_expressions.csv\" \n",
    "chat_expressions = pd.read_csv(chat_expressions_url, on_bad_lines='skip')\n",
    "chat_expressions_dict = dict(zip(chat_expressions.Chat_Words, chat_expressions.Chat_Words_Extended))\n",
    "\n",
    "all_emoji_emoticons = {**EMOTICONS_EMO,**UNICODE_EMOJI_ALIAS, **UNICODE_EMOJI_ALIAS}\n",
    "all_emoji_emoticons = {k:v.replace(\":\",\"\").replace(\"_\",\" \").strip() for k,v in all_emoji_emoticons.items()}\n",
    "kp_all_emoji_emoticons = KeywordProcessor()\n",
    "for k,v in all_emoji_emoticons.items():\n",
    "    kp_all_emoji_emoticons.add_keyword(k, v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(dataFrame):\n",
    "    for index, row in dataFrame.iterrows():\n",
    "        dataFrame.at[index, 'Text'] = ConvertEmojisAndEmoticonsToText(dataFrame.at[index, 'Text'], kp_all_emoji_emoticons)\n",
    "        dataFrame.at[index, 'Text'] = UnwrapSlangExpressions(dataFrame.at[index, 'Text'], chat_expressions_dict)\n",
    "        dataFrame.at[index, 'Text'] = RemoveContractions(dataFrame.at[index, 'Text'])\n",
    "        dataFrame.at[index, 'Text'] = Tokenize(dataFrame.at[index, 'Text'])\n",
    "        dataFrame.at[index, 'Text'] = RemoveUrls(dataFrame.at[index, 'Text'], httpUrls, noHttpUrls)\n",
    "        dataFrame.at[index, 'Text'] = RemoveStopWords(dataFrame.at[index, 'Text'])\n",
    "        dataFrame.at[index, 'Text'] = RemovePuncuation(dataFrame.at[index, 'Text'])\n",
    "        dataFrame.at[index, 'Text'] = Lemmatize(dataFrame.at[index, 'Text'], lemmatizer)\n",
    "        dataFrame.at[index, 'Text'] = ' '.join(dataFrame.at[index, 'Text'])\n",
    "        dataFrame.at[index, 'Text'] = RemoveNonEnglishRecord(dataFrame.at[index, 'Text'])\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessPartTwo(dataFrame):\n",
    "    rareWords, frequentWords = FindRareAndFrequentWords(dataFrame, minThreshhold=1)\n",
    "    for index, row in dataFrame.iterrows():\n",
    "        dataFrame.at[index, 'Text'] = Tokenize(dataFrame.at[index, 'Text'])\n",
    "        dataFrame.at[index, 'Text'] = RemoveRareAndFrequentWords(dataFrame.at[index, 'Text'], rareWords, frequentWords)\n",
    "        dataFrame.at[index, 'Text'] = ' '.join(dataFrame.at[index, 'Text'])\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClearEmptyValues(dataFrame):\n",
    "    dataFrame = dataFrame[dataFrame.Text != '']\n",
    "    dataFrame.dropna(how='any', inplace=True)\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WritePreprocessedData(prepprocessedData, fileName):\n",
    "    prepprocessedData.to_csv('Datasets/ProcessedData/' + fileName + '.csv', columns=['Emotion', 'Text'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessDataSet(dataFrame, fileName):\n",
    "    dataFrame = (\n",
    "    dataFrame\n",
    "    .pipe(Preprocess)\n",
    "    .pipe(ClearEmptyValues)\n",
    "    .pipe(PreprocessPartTwo)\n",
    "    .pipe(ClearEmptyValues))\n",
    "\n",
    "    WritePreprocessedData(dataFrame, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksandar\\AppData\\Local\\Temp\\ipykernel_22492\\3523428849.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataFrame.dropna(how='any', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "binaryDataset = pd.read_csv(\"Datasets/RawData/binary-sentiment.csv\")\n",
    "linesBinaryDataset = binaryDataset.reset_index()\n",
    "print(len(linesBinaryDataset))\n",
    "ProcessDataSet(linesBinaryDataset, 'binaryPreprocessedData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksandar\\AppData\\Local\\Temp\\ipykernel_22492\\3523428849.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataFrame.dropna(how='any', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "linesmultiClassDataset = pd.read_csv('Datasets/RawData/merged.csv')\n",
    "linesmultiClassDataset = linesmultiClassDataset.reset_index()\n",
    "ProcessDataSet(linesmultiClassDataset, 'multiClassPreprocessedData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTrainTestSplit(datasetType):\n",
    "    df = pd.read_csv('Datasets/ProcessedData/' + datasetType + '.csv')\n",
    "    df = df.reset_index()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Emotion, \n",
    "                            random_state=22,\n",
    "                            test_size=0.3, shuffle=True)\n",
    "\n",
    "    train_tuples = list(zip(y_train,X_train))\n",
    "    train_df = pd.DataFrame(train_tuples, columns=[\"Emotion\", \"Text\"])\n",
    "\n",
    "    test_tuples = list(zip(y_test,X_test))\n",
    "    test_df = pd.DataFrame(test_tuples, columns=[\"Emotion\", \"Text\"])\n",
    "\n",
    "    WritePreprocessedData(train_df, \"Train/\" + datasetType)\n",
    "    WritePreprocessedData(test_df, \"Test/\" + datasetType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateTrainTestSplit(\"binaryPreprocessedData\")\n",
    "CreateTrainTestSplit(\"multiClassPreprocessedData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NTRI_venv",
   "language": "python",
   "name": "ntri_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1562d732408e526937b0ee2222a31280c8ef74fc6afff3d9d3a2b3f8a7024a6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
